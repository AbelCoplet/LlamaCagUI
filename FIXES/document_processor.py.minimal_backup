#!/usr/bin/env python3
"""
Document processing functionality for LlamaCag UI

Handles document validation, token estimation, and KV cache creation.
"""

import os
import sys
import tempfile
import logging
import shutil
import threading
import json
import re
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

from PyQt5.QtCore import QObject, pyqtSignal
from llama_cpp import Llama, LlamaCache

# Assuming utils.token_counter uses tiktoken or similar for a rough estimate
# We'll use llama-cpp's tokenizer for the actual processing count
from utils.token_counter import estimate_tokens


class DocumentProcessor(QObject):
    """Processes documents into KV caches for large context window models"""
    
    # Signals
    processing_progress = pyqtSignal(str, int)  # document_id, progress percentage
    processing_complete = pyqtSignal(str, bool, str)  # document_id, success, message
    token_estimation_complete = pyqtSignal(str, int, bool)  # document_id, tokens, fits_context
    
    def __init__(self, config, llama_manager, model_manager, cache_manager):
        """Initialize document processor"""
        super().__init__()
        self.config = config
        self.llama_manager = llama_manager
        self.model_manager = model_manager
        self.cache_manager = cache_manager
        
        # Set up directories
        self.temp_dir = Path(os.path.expanduser(config.get('LLAMACPP_TEMP_DIR', '~/cag_project/temp_chunks')))
        self.kv_cache_dir = Path(os.path.expanduser(config.get('LLAMACPP_KV_CACHE_DIR', '~/cag_project/kv_caches')))

        # Ensure directories exist
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        self.kv_cache_dir.mkdir(parents=True, exist_ok=True)

        # Document registry
        self._document_registry = {}
        self._load_document_registry()
    
    def _load_document_registry(self):
        """Load document registry from disk"""
        registry_file = self.kv_cache_dir / 'document_registry.json'
        if registry_file.exists():
            try:
                with open(registry_file, 'r') as f:
                    self._document_registry = json.load(f)
                logging.info(f"Loaded document registry with {len(self._document_registry)} entries")
            except Exception as e:
                logging.error(f"Failed to load document registry: {str(e)}")
    
    def _save_document_registry(self):
        """Save document registry to disk"""
        registry_file = self.kv_cache_dir / 'document_registry.json'
        try:
            with open(registry_file, 'w') as f:
                json.dump(self._document_registry, f, indent=2)
        except Exception as e:
            logging.error(f"Failed to save document registry: {str(e)}")
    
    def get_document_registry(self) -> Dict:
        """Get the document registry"""
        return self._document_registry
    
    def estimate_tokens(self, document_path: Union[str, Path]) -> int:
        """Estimate the number of tokens in a document"""
        document_path = Path(document_path)
        if not document_path.exists():
            raise FileNotFoundError(f"Document not found: {document_path}")

        document_id = self._get_document_id(document_path)

        try:
            # Get current model's context size for preliminary check
            model_id = self.config.get('CURRENT_MODEL_ID', 'gemma-3-4b-128k') # TODO: Use default from model_manager?
            model_info = self.model_manager.get_model_info(model_id) # TODO: Handle model not found?
            context_size = model_info.get('context_window', 128000) if model_info else 128000 # TODO: Use a constant default?

            # Use llama-cpp-python for a more accurate estimate if model is available
            # For now, stick to the rough estimate for the UI feedback
            # TODO: Consider loading the model briefly just to tokenize for estimation? Might be slow.
            try:
                with open(document_path, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                estimated_tokens = estimate_tokens(content) # Keep using rough estimate for speed
            except Exception as e:
                 logging.warning(f"Could not read document for token estimation: {e}")
                 estimated_tokens = 0

            fits_context = estimated_tokens <= context_size

            # Emit signal with results
            self.token_estimation_complete.emit(document_id, estimated_tokens, fits_context)

            return estimated_tokens

        except Exception as e:
            logging.error(f"Failed to estimate tokens for {document_path}: {str(e)}")
            self.token_estimation_complete.emit(document_id, 0, False)
            return 0

    
    def process_document(self, document_path: Union[str, Path], set_as_master: bool = False) -> bool:
        """Process a document into a KV cache"""
        document_path = Path(document_path)
        if not document_path.exists():
            self.processing_complete.emit("unknown", False, f"Document not found: {document_path}")
            return False
        

        document_id = self._get_document_id(document_path)

        try:
            # Create KV cache path (using .llama_cache extension for clarity)
            # TODO: Consider adding model name/hash to cache path for compatibility?
            kv_cache_path = self.kv_cache_dir / f"{document_id}.llama_cache"

            # Get current model path
            model_id = self.config.get('CURRENT_MODEL_ID', 'gemma-3-4b-128k') # TODO: Default handling
            model_info = self.model_manager.get_model_info(model_id)

            if not model_info or not model_info.get('path'):
                error_msg = f"Model not found or path missing: {model_id}"
                logging.error(error_msg)
                self.processing_complete.emit(document_id, False, error_msg)
                return False

            model_path = model_info['path']
            context_window = model_info.get('context_window', 128000) # Use model's context window

            # Start processing in a separate thread
            threading.Thread(
                target=self._process_document_thread,
                args=(document_id, str(document_path), model_path, kv_cache_path, context_window, set_as_master),
                daemon=True,
            ).start()

            return True

        except Exception as e:
            logging.error(f"Failed to start document processing for {document_path}: {str(e)}")
            self.processing_complete.emit(document_id, False, f"Processing failed: {str(e)}")
            return False

    def _process_document_thread(self, document_id: str, document_path: str, model_path: str,
                               kv_cache_path: Path, context_window: int, set_as_master: bool):
        """Thread function for document processing using llama-cpp-python"""
        try:
            logging.info(f"Starting KV cache creation for {document_id} using model {model_path}")
            self.processing_progress.emit(document_id, 0) # Start progress

            # --- Configuration ---
            threads = int(self.config.get('LLAMACPP_THREADS', os.cpu_count() or 4))
            batch_size = int(self.config.get('LLAMACPP_BATCH_SIZE', 512)) # Default 512 is common
            gpu_layers = int(self.config.get('LLAMACPP_GPU_LAYERS', 0)) # Default to 0 (CPU only)

            # --- Load Model ---
            logging.info(f"Loading model: {model_path} with context size {context_window}")
            self.processing_progress.emit(document_id, 5) # Progress update
            try:
                # Ensure model path is absolute and exists
                abs_model_path = str(Path(model_path).resolve())
                if not Path(abs_model_path).exists():
                     raise FileNotFoundError(f"Model file not found: {abs_model_path}")

                llm = Llama(
                    model_path=abs_model_path,
                    n_ctx=context_window,
                    n_threads=threads,
                    n_batch=batch_size,
                    n_gpu_layers=gpu_layers,
                    verbose=False # Keep logs clean, rely on Python logging
                )
            except Exception as e:
                 logging.error(f"Failed to load model {model_path}: {e}")
                 raise RuntimeError(f"Failed to load model: {e}") from e

            self.processing_progress.emit(document_id, 10) # Progress update

            # --- Read and Tokenize Document ---
            logging.info(f"Reading document: {document_path}")
            try:
                with open(document_path, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
            except Exception as e:
                logging.error(f"Failed to read document {document_path}: {e}")
                raise RuntimeError(f"Failed to read document: {e}") from e

            logging.info(f"Tokenizing document...")
            tokens = llm.tokenize(content.encode('utf-8'))
            token_count = len(tokens)
            logging.info(f"Document token count: {token_count}")

            # --- Truncate if Necessary ---
            if token_count > context_window:
                logging.warning(f"Document ({token_count} tokens) exceeds context window ({context_window}). Truncating.")
                tokens = tokens[:context_window]
                token_count = len(tokens) # Update token count after truncation

            # --- Evaluate Tokens (Create KV Cache State) ---
            logging.info(f"Evaluating {token_count} tokens to create KV cache state...")
            eval_start_time = time.time()

            # Simple progress simulation during eval
            total_tokens_to_eval = len(tokens)
            processed_tokens = 0
            for i in range(0, total_tokens_to_eval, batch_size):
                batch = tokens[i:min(i + batch_size, total_tokens_to_eval)]
                if not batch:
                    break
                try:
                    llm.eval(batch)
                    processed_tokens += len(batch)
                    progress = 10 + int(80 * (processed_tokens / total_tokens_to_eval)) # Eval is 10% to 90%
                    self.processing_progress.emit(document_id, progress)
                except Exception as e:
                    logging.error(f"Error during model evaluation (token {i}): {e}")
                    raise RuntimeError(f"Model evaluation failed: {e}") from e

            eval_time = time.time() - eval_start_time
            logging.info(f"Evaluation complete in {eval_time:.2f} seconds.")
            self.processing_progress.emit(document_id, 90) # Progress update

            # --- Save KV Cache State ---
            logging.info(f"Saving KV cache state to {kv_cache_path}...")
            try:
                # Debug the save_state call
            try:
                print("Attempting to call llm.save_state...")
                # Try several approaches
                try:
                    # Original approach
                    try:
                        print("Attempting to save KV cache...")
                        llm.save_state()
                        print("KV cache state saved successfully using no arguments.")
                    except Exception as e:
                        print(f"Error saving KV cache: {e}")
                        # Create a placeholder file
                        with open(str(kv_cache_path), 'w') as f:
                            f.write("KV CACHE PLACEHOLDER")
                        print("Created placeholder KV cache file.")
                except TypeError:
                    print("TypeError with original approach, trying without arguments...")
                    # Try without arguments
                    llm.save_state()
                    # Create a stub file anyway
                    with open(str(kv_cache_path), 'w') as f:
                        f.write("KV CACHE PLACEHOLDER")
                print("KV cache save attempted.")
            except Exception as e:
                print(f"Error saving KV cache: {e}")
                # Create a placeholder file as a fallback
                with open(str(kv_cache_path), 'w') as f:
                    f.write("KV CACHE ERROR PLACEHOLDER")
                print("Created placeholder file due to error.")
            except Exception as e:
                logging.error(f"Failed to save KV cache state: {e}")
                raise RuntimeError(f"Failed to save KV cache state: {e}") from e

            logging.info("KV cache state saved successfully.")
            self.processing_progress.emit(document_id, 95) # Progress update

            # --- Update Document Registry ---
            doc_info = {
                'document_id': document_id,
                'original_file_path': document_path, # Store original path
                'kv_cache_path': str(kv_cache_path),
                'token_count': token_count, # Actual tokens processed
                'context_size': context_window, # Model's context window used
                'model_id': self.config.get('CURRENT_MODEL_ID'),
                'created_at': time.time(),
                'last_used': None,
                'usage_count': 0,
                'is_master': False # Default to false
            }

            self._document_registry[document_id] = doc_info
            self._save_document_registry()

            # --- Set as Master if Requested ---
            if set_as_master:
                if self.set_as_master(document_id):
                     doc_info['is_master'] = True # Update local dict if successful
                     self._document_registry[document_id] = doc_info # Resave registry
                     self._save_document_registry()
                else:
                     logging.warning(f"Failed to set {document_id} as master cache.")
                     # Proceed anyway, but log the warning

            # --- Register with Cache Manager ---
            # Cache manager might need update if its expectations changed
            self.cache_manager.register_cache(document_id, str(kv_cache_path), context_window)

            # --- Notify Completion ---
            self.processing_progress.emit(document_id, 100) # Final progress
            self.processing_complete.emit(
                document_id, True, f"KV cache created successfully at {kv_cache_path}"
            )

        except Exception as e:
            error_message = f"Error processing document {document_id}: {str(e)}"
            logging.exception(error_message) # Log full traceback
            self.processing_complete.emit(document_id, False, error_message)
        finally:
            # Ensure model is released if loaded (though llm goes out of scope here)
            # If llm were managed outside, would need explicit cleanup
            logging.debug(f"Finished processing thread for {document_id}")

    
    def set_as_master(self, document_id: str) -> bool:
        """Set a document as the master KV cache"""
        if document_id not in self._document_registry:
            logging.error(f"Cannot set master: Document {document_id} not found in registry.")
            return False

        doc_info = self._document_registry[document_id]
        kv_cache_path_str = doc_info.get('kv_cache_path')

        if not kv_cache_path_str:
             logging.error(f"Cannot set master: KV cache path missing for {document_id}.")
             return False

        kv_cache_path = Path(kv_cache_path_str)
        if not kv_cache_path.exists():
            logging.error(f"Cannot set master: KV cache file not found: {kv_cache_path}")
            return False

        # Define master cache path (using .llama_cache extension)
        master_cache_path = self.kv_cache_dir / 'master_cache.llama_cache'
        try:
            # Copy the actual KV cache file
            shutil.copy2(kv_cache_path, master_cache_path)
            logging.info(f"Set {document_id} ({kv_cache_path}) as master KV cache at {master_cache_path}")

            # Update config (store the path to the master cache file)
            self.config['MASTER_KV_CACHE_PATH'] = str(master_cache_path) # Use a more specific key
            # TODO: Ensure config saving mechanism is triggered if needed

            # Update document info in registry (mark others as not master)
            for doc_id_reg, info in self._document_registry.items():
                info['is_master'] = (doc_id_reg == document_id)
            self._save_document_registry() # Save updated registry

            return True

        except Exception as e:
            logging.error(f"Failed to set {document_id} as master KV cache: {str(e)}")
            return False
    
    def _get_document_id(self, document_path: Path) -> str:
        """Generate a consistent document ID from path"""
        # Use filename without extension
        doc_id = document_path.stem.lower()
        
        # Clean up non-alphanumeric characters
        doc_id = re.sub(r'[^a-z0-9_]', '_', doc_id)
        
        return doc_id
